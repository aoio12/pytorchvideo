{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "from comet_ml import Experiment\n",
    "from ast import parse\n",
    "from logging import NullHandler, root\n",
    "import re\n",
    "from imgaug.augmenters import size\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataloader import default_collate\n",
    "from torch.utils.data import DistributedSampler, RandomSampler, SequentialSampler\n",
    "import numpy as np\n",
    "\n",
    "from torchvision import transforms\n",
    "\n",
    "from pytorchvideo.models import x3d\n",
    "from pytorchvideo.data import RandomClipSampler, UniformClipSampler\n",
    "from pytorchvideo.data import Ucf101, RandomClipSampler, UniformClipSampler, Kinetics\n",
    "# from segmentation_dataset import segUcf101, segkinetics\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "from torchvision.transforms.functional import scale\n",
    "\n",
    "from tqdm import tqdm\n",
    "from collections import OrderedDict\n",
    "import itertools\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import imageio\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "import configparser\n",
    "from fractions import Fraction\n",
    "\n",
    "from compression_transform import SameClipSampler\n",
    "\n",
    "# from compression_transform import (\n",
    "#     JpegCompression,\n",
    "#     CompCompose,\n",
    "#     BothCompose,\n",
    "#     CompApplyTransformToKey,\n",
    "#     CompUniformTemporalSubsample,\n",
    "#     CompLambda,\n",
    "#     BothLambda,\n",
    "#     CompNormalize,\n",
    "#     CompRandomShortSideScale,\n",
    "#     CompRandomCrop,\n",
    "#     CompRandomHorizontalFlip,\n",
    "#     CompRemoveKey,\n",
    "#     CompShortSideScale,\n",
    "#     CompCenterCrop,\n",
    "#     SameClipSampler\n",
    "# )\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "class LimitDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        super().__init__()\n",
    "        self.dataset = dataset\n",
    "        self.dataset_iter = itertools.chain.from_iterable(\n",
    "            itertools.repeat(iter(dataset), 2)\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return next(self.dataset_iter)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.dataset.num_videos"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "class JpegCompression():\n",
    "    def __init__(self, compression_strength, size_params):\n",
    "        self.compression_strength = compression_strength\n",
    "        self.size_params = size_params\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # args = Args()\n",
    "        aug = iaa.JpegCompression(self.compression_strength)\n",
    "        for i in range(int(self.size_params['NUM_FRAMES'])):\n",
    "            image = img[:, i, :, :].permute(1, 2, 0)\n",
    "            # xx = xx.to('cpu').detach().numpy().copy()\n",
    "            image = image.numpy()\n",
    "            # xx = xx * 255\n",
    "            # xx = xx.numpy()\n",
    "            image = image.clip(0, 255).astype(np.uint8)\n",
    "            # xx = xx.astype(np.uint8)\n",
    "            tmp = aug.augment_image(image)\n",
    "            tmp = np.asarray(tmp).astype('float32')\n",
    "            tmp = tmp / 255\n",
    "            img[:, i, :, :] = torch.from_numpy(tmp).permute(2, 0, 1)\n",
    "            # img[:,i,:,:] = torch.from_numpy(tmp).clone().permute(2,0,1)\n",
    "        return img\n",
    "\n",
    "class PackPathway(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Transform for converting video frames as a list of tensors.\n",
    "    https://pytorch.org/hub/facebookresearch_pytorchvideo_slowfast/\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.slowfast_alpha = 4\n",
    "\n",
    "    def forward(self, frames: torch.Tensor):\n",
    "        fast_pathway = frames\n",
    "        # Perform temporal sampling from the fast pathway.\n",
    "        slow_pathway = torch.index_select(\n",
    "            frames,\n",
    "            1,\n",
    "            torch.linspace(\n",
    "                0, frames.shape[1] - 1, frames.shape[1] // self.slowfast_alpha\n",
    "            ).long(),\n",
    "        )\n",
    "        frame_list = [slow_pathway, fast_pathway]\n",
    "        return frame_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "def use_compression(args, size_params):\n",
    "    compression_strength = args.compression_strength\n",
    "    en = args.use_compression\n",
    "    jpeg_compression = None\n",
    "    if (en):\n",
    "        print('圧縮がかかっています')\n",
    "        jpeg_compression = JpegCompression(\n",
    "            compression_strength, size_params)\n",
    "    else:\n",
    "        print('圧縮はかかっていません')\n",
    "        jpeg_compression = transforms.Lambda(lambda x: x / 255.)\n",
    "    return jpeg_compression\n",
    "\n",
    "\n",
    "def use_packpathway(args):\n",
    "    model_name = args.model_name\n",
    "    if \"slowfast\" in model_name:\n",
    "        packpathway = PackPathway()\n",
    "    else:\n",
    "        packpathway = transforms.Lambda(lambda x: x)\n",
    "    return packpathway"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def get_kinetics(args, jpeg_compression, size_params, packpathway):\n",
    "    \"\"\"\n",
    "    Kinetics400のデータセットを取得\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"val\"\n",
    "    Returns:\n",
    "        pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "\n",
    "    subset = args.subset\n",
    "    root_path = args.path\n",
    "    transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(\n",
    "                    int(size_params['NUM_FRAMES'])),\n",
    "                # print(config['NUM_WORKERS']),\n",
    "                # TODO: 受け渡されたjpegcompressionオブジェクトをここに入れる\n",
    "                jpeg_compression,\n",
    "                # transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                # RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                ShortSideScale(size=int(size_params['SIDE_SIZE'])),\n",
    "                CenterCrop(int(size_params['CROP_SIZE'])),\n",
    "                # RandomHorizontalFlip(),\n",
    "                # PackPathway(),\n",
    "                packpathway,\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    # root_kinetics = '/mnt/NAS-TVS872XT/dataset/Kinetics400/'\n",
    "    root_kinetics = root_path\n",
    "    # The duration of the input clip is also specific to the model.\n",
    "    num_frames = int(size_params['NUM_FRAMES'])\n",
    "    sampling_rate = int(size_params['SAMPLIMG_RATE'])\n",
    "    frames_per_second = 30\n",
    "    clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
    "\n",
    "    dataset = Kinetics(\n",
    "        data_path=root_kinetics + subset,\n",
    "        video_path_prefix=root_kinetics + subset,\n",
    "        clip_sampler=SameClipSampler(\n",
    "            clip_duration=float(clip_duration)),\n",
    "        video_sampler=SequentialSampler,\n",
    "        decode_audio=False,\n",
    "        transform=transform,\n",
    "    )\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def get_ucf101(subset, args, jpeg_compression, size_params, packpathway):\n",
    "    \"\"\"\n",
    "    ucf101のデータセットを取得\n",
    "    Args:\n",
    "        subset (str): \"train\" or \"val\"\n",
    "    Returns:\n",
    "        pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset: 取得したデータセット\n",
    "    \"\"\"\n",
    "\n",
    "    # subset = args.subset\n",
    "    root_path = args.path\n",
    "    subset_root_ucf101 = 'ucfTrainTestlist/trainlist01.txt'\n",
    "    if subset == \"test\":\n",
    "        subset_root_ucf101 = 'ucfTrainTestlist/testlist.txt'\n",
    "\n",
    "    # args = Args()\n",
    "    train_transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(int(size_params['NUM_FRAMES'])),\n",
    "                transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                RandomCrop(224),\n",
    "                RandomHorizontalFlip(),\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x - 1),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    val_transform = Compose([\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose([\n",
    "                UniformTemporalSubsample(\n",
    "                    int(size_params['NUM_FRAMES'])),\n",
    "                # print(config['NUM_WORKERS']),\n",
    "                # TODO: 受け渡されたjpegcompressionオブジェクトをここに入れる\n",
    "                jpeg_compression,\n",
    "                # transforms.Lambda(lambda x: x / 255.),\n",
    "                Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                # RandomShortSideScale(min_size=256, max_size=320,),\n",
    "                ShortSideScale(size=int(size_params['SIDE_SIZE'])),\n",
    "                CenterCrop(int(size_params['CROP_SIZE'])),\n",
    "                # RandomHorizontalFlip(),\n",
    "                # PackPathway(),\n",
    "                packpathway,\n",
    "            ]),\n",
    "        ),\n",
    "        ApplyTransformToKey(\n",
    "            key=\"label\",\n",
    "            transform=transforms.Lambda(lambda x: x - 1),\n",
    "        ),\n",
    "        RemoveKey(\"audio\"),\n",
    "    ])\n",
    "\n",
    "    transform = train_transform if subset == \"train\" else val_transform\n",
    "\n",
    "    root_ucf101 = root_path\n",
    "    # root_ucf101 = '/mnt/NAS-TVS872XT/dataset/UCF101/'\n",
    "\n",
    "    num_frames = int(size_params['NUM_FRAMES'])\n",
    "    sampling_rate = int(size_params['SAMPLIMG_RATE'])\n",
    "    frames_per_second = 30\n",
    "    clip_duration = (num_frames * sampling_rate)/frames_per_second\n",
    "\n",
    "    dataset = Ucf101(\n",
    "        data_path=root_ucf101 + subset_root_ucf101,\n",
    "        video_path_prefix=root_ucf101 + 'video/',\n",
    "        clip_sampler=SameClipSampler(\n",
    "            clip_duration=float(clip_duration)),\n",
    "        video_sampler=SequentialSampler,\n",
    "        decode_audio=False,\n",
    "        transform=transform,\n",
    "    )\n",
    "    return dataset\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "def make_loader(dataset, config):\n",
    "    \"\"\"\n",
    "    データローダーを作成\n",
    "    Args:\n",
    "        dataset (pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset): get_datasetメソッドで取得したdataset\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: 取得したデータローダー\n",
    "    \"\"\"\n",
    "    # args = Args()\n",
    "    loader = DataLoader(LimitDataset(dataset),\n",
    "                        batch_size=int(config['BATCH_SIZE']),\n",
    "                        drop_last=True,\n",
    "                        num_workers=int(config['NUM_WORKERS']))\n",
    "    return loader\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def get_dataset(args, jpeg_compression, size_params, pathpackway):\n",
    "    \"\"\"\n",
    "    データセットを取得\n",
    "    Args:\n",
    "        dataset (str): \"Kinetis400\" or \"UCF101\"\n",
    "        subset (str): \"train\" or \"val\"\n",
    "    Returns:\n",
    "        pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset): 取得したデータセット\n",
    "    \"\"\"\n",
    "    # TODO: get_kineticsにjpegcompressionを渡す\n",
    "    # print('get_dataset:', jpeg_compression)\n",
    "    dataset = args.dataset\n",
    "    print('dataset:', dataset)\n",
    "    # subset = args.subset\n",
    "    if dataset == \"Kinetics400\":\n",
    "        return get_kinetics(args, jpeg_compression, size_params, pathpackway)\n",
    "    elif dataset == \"UCF101\":\n",
    "        return get_ucf101(args, jpeg_compression, size_params, pathpackway)\n",
    "    else:\n",
    "        print('dataloader error')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "def get_model(args, pretrained):\n",
    "    \"\"\"\n",
    "    pytorchvideoからモデルを取得\n",
    "    Args:\n",
    "        model (str): \"x3d_m\"(UCF101用) or \"slow_r50\"(Kinetics400用)\n",
    "        pretrained (bool): \"True\" or \"False\"\n",
    "    Returns:\n",
    "        model: 取得したモデル\n",
    "    \"\"\"\n",
    "    model_name = args.model_name\n",
    "    dataset = args.dataset\n",
    "    model = torch.hub.load(\n",
    "        'facebookresearch/pytorchvideo', model_name, pretrained=pretrained)\n",
    "    # do_fine_tune = True\n",
    "    # if do_fine_tune:\n",
    "    #     for param in model.parameters():\n",
    "    #         param.requires_grad = False\n",
    "    if dataset == 'UCF101':\n",
    "        model.blocks[5].proj = nn.Linear(\n",
    "            model.blocks[5].proj.in_features, 101)\n",
    "    # if model_name == \"x3d_m\":\n",
    "    #     model.blocks[5].proj = nn.Linear(\n",
    "    #         model.blocks[5].proj.in_features, 400)\n",
    "    return model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "def dataset_check(dataset, subset):\n",
    "    \"\"\"\n",
    "    取得したローダーの挙動を確認\n",
    "    Args:\n",
    "        dataset (str): \"Kinetics400\" or \"UCF101\"\n",
    "        subset (str): \"train\" or \"val\"\n",
    "    \"\"\"\n",
    "    dataset = get_dataset(\"Kinetics400\", \"val\")\n",
    "    loader = make_loader(dataset)\n",
    "    print(\"len:{}\".format(len(loader)))\n",
    "    for i, batch in enumerate(loader):\n",
    "        if i == 0:\n",
    "            print(batch.keys())\n",
    "            print(batch['video'].shape)\n",
    "        print(batch['label'].cpu().numpy())\n",
    "        if i > 4:\n",
    "            break"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def get_params(config, size_params):\n",
    "    params = {\n",
    "        \"batch_size\": int(config['BATCH_SIZE']),\n",
    "        \"num_workers\": int(config['NUM_WORKERS']),\n",
    "        \"size_size\": int(size_params['SIDE_SIZE']),\n",
    "        \"crop_size\": int(size_params['CROP_SIZE']),\n",
    "        \"num_frames\": int(size_params['NUM_FRAMES']),\n",
    "        \"sampling_rate\": int(size_params['SAMPLIMG_RATE'])\n",
    "        # \"model\": config['MODEL']\n",
    "    }\n",
    "\n",
    "    return params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    Imported from https://github.com/pytorch/examples/blob/cedca7729fef11c91e28099a0e45d7e98d03b66d/imagenet/main.py#L363-L380\n",
    "    https://github.com/machine-perception-robotics-group/attention_branch_network/blob/ced1d97303792ac6d56442571d71bb0572b3efd8/utils/misc.py#L59\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        if type(val) == torch.Tensor:\n",
    "            val = val.item()\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"\n",
    "    Computes the accuracy over the k top predictions for the specified values of k\n",
    "    https://github.com/pytorch/examples/blob/cedca7729fef11c91e28099a0e45d7e98d03b66d/imagenet/main.py#L411\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def top1(outputs, targets):\n",
    "    batch_size = outputs.size(0)\n",
    "    _, predicted = outputs.max(1)\n",
    "    return predicted.eq(targets).sum().item() / batch_size"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "def train(args, config, size_params, experiment):\n",
    "    device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "    jpeg_compression = use_compression(args, size_params)\n",
    "    packpathway = use_packpathway(args)\n",
    "\n",
    "    train_dataset = get_ucf101(\"train\", args, jpeg_compression, size_params, packpathway)\n",
    "    val_dataset = get_ucf101(\"test\", args, jpeg_compression, size_params, packpathway)\n",
    "    train_loader = make_loader(train_dataset, config)\n",
    "    val_loader = make_loader(val_dataset, config)\n",
    "\n",
    "    model = get_model(args, True)\n",
    "    model = model.to(device)\n",
    "    # model = torch.nn.DataParallel(model)\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    lr = 0.01\n",
    "    optimizer = torch.optim.SGD(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        momentum=0.9,\n",
    "        weight_decay=5e-4)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # hyper_params = {\n",
    "    #     \"Dataset\": \"UCF101\",\n",
    "    #     \"epoch\": args.NUM_EPOCH,\n",
    "    #     \"batch_size\": args.BATCH_SIZE,\n",
    "    #     \"num_frame\": args.VIDEO_NUM_SUBSAMPLED,\n",
    "    #     \"learning late\": lr,\n",
    "    #     \"mode\": \"finetune\",\n",
    "    #     \"Adapter\": \"adp:0, adp:1\",\n",
    "    # }\n",
    "\n",
    "    # experiment = Experiment(\n",
    "    #     api_key=\"TawRAwNJiQjPaSMvBAwk4L4pF\",\n",
    "    #     project_name=\"feeature-extract\",\n",
    "    #     workspace=\"kazukiomi\",\n",
    "    # )\n",
    "\n",
    "    # experiment.add_tag('pytorch')\n",
    "    # experiment.log_parameters(hyper_params)\n",
    "\n",
    "    num_epochs = int(config['NUM_EPOCH'])\n",
    "    \n",
    "\n",
    "    step = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "        for epoch in pbar_epoch:\n",
    "            pbar_epoch.set_description(\"[Epoch %d]\" % (epoch))\n",
    "\n",
    "            \"\"\"Training mode\"\"\"\n",
    "\n",
    "            train_loss = AverageMeter()\n",
    "            train_acc = AverageMeter()\n",
    "\n",
    "            with tqdm(enumerate(train_loader),\n",
    "                      total=len(train_loader),\n",
    "                      leave=True) as pbar_train_batch:\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                for batch_idx, batch in pbar_train_batch:\n",
    "                    pbar_train_batch.set_description(\n",
    "                        \"[Epoch :{}]\".format(epoch))\n",
    "\n",
    "                    inputs = batch['video'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    train_loss.update(loss, bs)\n",
    "                    train_acc.update(top1(outputs, labels), bs)\n",
    "\n",
    "                    pbar_train_batch.set_postfix_str(\n",
    "                        ' | loss_avg={:6.04f} , top1_avg={:6.04f}'\n",
    "                        ' | batch_loss={:6.04f} , batch_top1={:6.04f}'\n",
    "                        ''.format(\n",
    "                            train_loss.avg, train_acc.avg,\n",
    "                            train_loss.val, train_acc.val,\n",
    "                        ))\n",
    "\n",
    "                    experiment.log_metric(\n",
    "                        \"batch_accuracy\", train_acc.val, step=step)\n",
    "                    step += 1\n",
    "\n",
    "            \"\"\"Val mode\"\"\"\n",
    "            model.eval()\n",
    "            val_loss = AverageMeter()\n",
    "            val_acc = AverageMeter()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, val_batch in enumerate(val_loader):\n",
    "                    inputs = val_batch['video'].to(device)\n",
    "                    labels = val_batch['label'].to(device)\n",
    "\n",
    "                    bs = inputs.size(0)\n",
    "\n",
    "                    val_outputs = model(inputs)\n",
    "                    loss = criterion(val_outputs, labels)\n",
    "\n",
    "                    val_loss.update(loss, bs)\n",
    "                    val_acc.update(top1(val_outputs, labels), bs)\n",
    "            \"\"\"Finish Val mode\"\"\"\n",
    "\n",
    "            \"\"\"save model\"\"\"\n",
    "            if best_acc < val_acc.avg:\n",
    "                best_acc = val_acc.avg\n",
    "                is_best = True\n",
    "            else:\n",
    "                is_best = False\n",
    "                \n",
    "            # save_checkpoint(model, is_best, filename=\"finetune_checkpoint.pth\", best_model_file=\"finetune_best.pth\", dir_data_name=\"adapter_2d/UCF101\")\n",
    "            \n",
    "\n",
    "            pbar_epoch.set_postfix_str(\n",
    "                ' train_loss={:6.04f} , val_loss={:6.04f}, train_acc={:6.04f}, val_acc={:6.04f}'\n",
    "                ''.format(\n",
    "                    train_loss.avg,\n",
    "                    val_loss.avg,\n",
    "                    train_acc.avg,\n",
    "                    val_acc.avg)\n",
    "            )\n",
    "\n",
    "            experiment.log_metric(\"epoch_train_accuracy\",\n",
    "                                  train_acc.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"epoch_train_loss\",\n",
    "                                  train_loss.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_accuracy\",\n",
    "                                  val_acc.avg,\n",
    "                                  step=epoch + 1)\n",
    "            experiment.log_metric(\"val_loss\",\n",
    "                                  val_loss.avg,\n",
    "                                  step=epoch + 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "def sample_check(args, config, experiment, size_params):\n",
    "    \"\"\"学習済みモデルにサンプルデータ100個を流し込んで挙動を確認\"\"\"\n",
    "    device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = get_model(args, True)\n",
    "    model = model.to(device)\n",
    "    # TODO: jpegcompressionオブジェクトここで作ってget_datasetに渡す\n",
    "    # jpeg_compression = JpegCompression\n",
    "    # compression = jpeg_compression()\n",
    "    # print(\"sample_check:\", compression)\n",
    "    jpeg_compression = use_compression(args, size_params)\n",
    "    pathpackway = use_packpathway(args)\n",
    "    # print('type(jpeg_compression):', type(jpeg_compression))\n",
    "    dataset = get_dataset(args, jpeg_compression, size_params, pathpackway)\n",
    "\n",
    "    # dataset.video_sampler._num_samples = 100\n",
    "    sample_loader = make_loader(dataset, config)\n",
    "\n",
    "    acc1_list = []\n",
    "    acc5_list = []\n",
    "    log_top1 = AverageMeter()\n",
    "    log_top5 = AverageMeter()\n",
    "    video_num = 0\n",
    "    model.eval()\n",
    "    with experiment.validate(), torch.no_grad():\n",
    "        with tqdm(enumerate(sample_loader),\n",
    "                  total=len(sample_loader),\n",
    "                  leave=True) as pbar_batch:\n",
    "\n",
    "            for batch_idx, batch in pbar_batch:\n",
    "                # print(batch['video_name'])\n",
    "                # print(batch['video'].shape)\n",
    "                # print('batch_idx:', batch_idx)\n",
    "                # print(1)\n",
    "                # sub_inputs = PackPathway(batch[\"video\"])\n",
    "                # inputs = [b.to(device) for b in sub_inputs]\n",
    "                if \"slowfast\" in args.model_name:\n",
    "                    inputs = [b.to(device) for b in batch['video']]\n",
    "                    experiment.log_parameter(\n",
    "                        'num_frame_0', inputs[0][0, 0, :, :, :].shape)\n",
    "                    experiment.log_parameter(\n",
    "                        'num_frame_1', inputs[1][0, 0, :, :, :].shape)\n",
    "                    current_batch_size = inputs[0].size()[0]\n",
    "                else:\n",
    "                    inputs = batch[\"video\"].to(device)\n",
    "                    experiment.log_parameter(\n",
    "                        'frame_size_0', inputs[0, 0, 0, :, :].shape)\n",
    "                    # print('shape:', inputs[:, 0, 0, 0, 0].shape)\n",
    "                    current_batch_size = inputs.size()[0]\n",
    "                    # print('inputs.shape:', inputs.size()[0])\n",
    "                    # print(inputs[:, 0, 0, 0, 0].shape)\n",
    "\n",
    "                # inputs = [b.to(device) for b in batch['video']]\n",
    "                labels = batch[\"label\"].to(device)\n",
    "                outputs = model(inputs)\n",
    "                # print(inputs[0, 0, 0, :, :].shape)\n",
    "                # print(inputs.shape[4])\n",
    "                # preds = torch.squeeze(outputs.max(dim=1)[1])\n",
    "                acc1, acc5 = accuracy(outputs, labels, topk=(1, 5))\n",
    "                log_top1.update(acc1, current_batch_size)\n",
    "                log_top5.update(acc5, current_batch_size)\n",
    "\n",
    "                acc1_list.append(log_top1.avg)\n",
    "                acc5_list.append(log_top5.avg)\n",
    "                pbar_batch.set_postfix_str(' | acc top1={:6.04f} top5={:6.04f}'''\n",
    "                                           .format(log_top1.avg, log_top5.avg))\n",
    "                video_num += current_batch_size\n",
    "                experiment.log_parameter('batch_idx', batch_idx)\n",
    "\n",
    "        accuracy1 = sum(acc1_list) / len(acc1_list)\n",
    "        accuracy5 = sum(acc5_list) / len(acc5_list)\n",
    "        print(\"sum_acc1:{}, sum_acc5:{}\".format(\n",
    "            sum(acc1_list), sum(acc5_list)))\n",
    "        print(\"len_acc1:{}, len_acc5:{}\".format(\n",
    "            len(acc1_list), len(acc5_list)))\n",
    "        print(\"acc1:{}, acc5:{}\".format(accuracy1, accuracy5))\n",
    "        experiment.log_metric(\"acc1\", accuracy1)\n",
    "        experiment.log_metric(\"acc5\", accuracy5)\n",
    "        experiment.log_parameter('video_num', video_num)\n",
    "\n",
    "    experiment.end()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "def main():\n",
    "    # Create an experiment with your api key\n",
    "    # experiment = Experiment(\n",
    "    #     api_key=\"VaG6pF4qhcqKJOux0daNkIz2C\",\n",
    "    #     project_name=\"jpeg-compression\",\n",
    "    #     workspace=\"ohtani\",\n",
    "    # )\n",
    "\n",
    "    experiment = Experiment(\n",
    "        api_key=\"VaG6pF4qhcqKJOux0daNkIz2C\",\n",
    "        project_name=\"ffmpeg\",\n",
    "        workspace=\"ohtani\",\n",
    "    )\n",
    "\n",
    "    # print(config['CLIP_DURATION'])\n",
    "    # print(type(config['CLIP_DURATION']))\n",
    "    # TODO: argparseをここに入れる\n",
    "    # parser = argparse.ArgumentParser(description='jpegcompressionの圧縮の強さ')\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--dataset', action='store',\n",
    "                        default='Kinetics400', help='')\n",
    "    parser.add_argument('-sb', '--subset',\n",
    "                        action='store', default='val', help='')\n",
    "    # parser.add_argument('--path', action='store',\n",
    "    #                     default=config['DATA_PATH'],\n",
    "    #                     help='file path it is written urls')\n",
    "    parser.add_argument('--use_compression', action='store_true', help='')\n",
    "    parser.add_argument('-cs', '--compression_strength', type=float,\n",
    "                        default=99.0, help='Please Enter a number')\n",
    "    parser.add_argument('--model_name', action='store',\n",
    "                        default='x3d_m', help='Enter model name')\n",
    "    parser.add_argument('--q_use', action='store_true', help=' ')\n",
    "    \n",
    "    args = parser.parse_args(args=['--dataset','UCF101','-sb','train'])\n",
    "    \n",
    "\n",
    "    # simple_checkの引数にargparseを入れる\n",
    "    compression_strength = args.compression_strength\n",
    "    print('compression_strength:', compression_strength)\n",
    "\n",
    "    config = configparser.ConfigParser()\n",
    "    config.read('config.ini')\n",
    "    config = config[args.dataset]\n",
    "    parser.add_argument('--path', action='store',\n",
    "                        default=config['DATA_PATH'],\n",
    "                        help='file path it is written urls')\n",
    "    \n",
    "\n",
    "    #notebook用\n",
    "    # args = parser.parse_args()\n",
    "    args = parser.parse_args(args=['--dataset','UCF101','-sb','train'])\n",
    "\n",
    "    root_path = args.path\n",
    "    # root_path = config['DATA_PATH']\n",
    "    if os.path.exists(root_path):\n",
    "        print('PATH:', root_path)\n",
    "    else:\n",
    "        print('No File')\n",
    "\n",
    "    en = args.use_compression\n",
    "    print('en:', en)\n",
    "\n",
    "    model_name = args.model_name\n",
    "    experiment.log_parameter('model', model_name)\n",
    "    print('model_name:', model_name)\n",
    "\n",
    "    size_params = configparser.ConfigParser()\n",
    "    size_params.read('model.ini')\n",
    "    size_params = size_params[model_name]\n",
    "\n",
    "    params = get_params(config, size_params)\n",
    "    experiment.log_parameters(params)\n",
    "    if(en):\n",
    "        experiment.log_parameter('compression', compression_strength)\n",
    "    else:\n",
    "        experiment.log_parameter('compression', -10)\n",
    "    experiment.log_parameter('path_root', root_path)\n",
    "\n",
    "    experiment.add_tag('test')\n",
    "\n",
    "    if args.q_use:\n",
    "        q = os.path.split(os.path.split(root_path)[0])[1]\n",
    "        experiment.log_parameter('q', q)\n",
    "        print('q:', q)\n",
    "    experiment.log_parameter('q', 0)\n",
    "\n",
    "    # sample_check(args, config, experiment, size_params)\n",
    "    train(args, config, size_params, experiment)\n",
    "    experiment.end()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.ml/ohtani/ffmpeg/7f85cc4b6d684a04b1edcb008e214d94\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     batch_size    : 10\n",
      "COMET INFO:     compression   : -10\n",
      "COMET INFO:     crop_size     : 224\n",
      "COMET INFO:     model         : x3d_m\n",
      "COMET INFO:     num_frames    : 16\n",
      "COMET INFO:     num_workers   : 8\n",
      "COMET INFO:     path_root     : /mnt/dataset/UCF101/\n",
      "COMET INFO:     q             : 0\n",
      "COMET INFO:     sampling_rate : 5\n",
      "COMET INFO:     size_size     : 256\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (142.01 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     os packages              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/ohtani/ffmpeg/2c2d6435572d4c8995df593e558006c2\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "compression_strength: 99.0\n",
      "PATH: /mnt/dataset/UCF101/\n",
      "en: False\n",
      "model_name: x3d_m\n",
      "圧縮はかかっていません\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Using cache found in /home/ohtani/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "[Epoch :0]:  32%|███▏      | 306/953 [04:00<08:28,  1.27it/s,  | loss_avg=4.6140 , top1_avg=0.0281 | batch_loss=4.6161 , batch_top1=0.0000]\n",
      "[Epoch 0]:   0%|          | 0/5 [04:01<?, ?it/s]\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_156516/217905245.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_156516/2023662575.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;31m# sample_check(args, config, experiment, size_params)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_156516/687626238.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, config, size_params, experiment)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorchvideo/models/net.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorchvideo/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1384\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_block\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mres_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1386\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1387\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorchvideo/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1171\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch1_norm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1172\u001b[0m                 \u001b[0mshortcut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch1_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1173\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch_fusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshortcut\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbranch2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1175\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/pytorchvideo/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1342\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_b\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_b\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_b\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/fvcore/nn/squeeze_excitation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_tensor)\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         )\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    589\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 590\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    583\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             )\n\u001b[0;32m--> 585\u001b[0;31m         return F.conv3d(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "98b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}